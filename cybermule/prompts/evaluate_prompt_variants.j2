{# File: prompts/evaluate_prompt_variants.j2 #}

You are a prompt evaluator. Your job is to compare the quality of different responses generated by an AI model, each using a different prompt variant. Your evaluation goal is:

**Goal:** {{ goal }}

Below are the full input prompts (including conversation history) and the AI's responses.

Please do the following:
1. Assign a score from 1 (worst) to 5 (best) for each variant.
2. Select the best variant.
3. Justify your decision briefly.

{% for variant in variants %}
### Variant {{ loop.index }} (Node ID: {{ variant.node_id }})

**Full Prompt (including conversation):**
<full_prompt>
{{ variant.full_prompt }}
</full_prompt>

**Response:**
<response>
{{ variant.response }}
</response>

{% endfor %}

Return your answer as a JSON object:
```json
{
  "scores": {
    {% for variant in variants %}
    "{{ variant.node_id }}": <score>{{ "," if not loop.last }}
    {% endfor %}
  },
  "winner": "<best_node_id>",
  "justification": "<brief reasoning here>"
}
```
